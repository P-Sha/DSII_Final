---
title: "test - james - trees"
author: "James Ng (nj2208)"
date: "5/5/2021"
output: html_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(RNHANES)
library(caret)
#library(glmnet)
#library(pls)
#library(splines)
#library(mgcv)
#library(pdp)
#library(earth)
#library(ggplot2)
#library(lasso2)
#library(mlbench)
#library(pROC)
#library(vip)
#library(AppliedPredictiveModeling)
#library(qwraps2)
#library(arsenal)
#library(dplyr)
#library(HH)
#library(summarytools)
#library(leaps)


knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
data_files <- nhanes_load_data(file_name = "DIQ_H", year = "2013-2014")

data_files <- data_files %>% 
  left_join(nhanes_load_data("HDL_H", "2013-2014"), by = "SEQN") %>%
  left_join(nhanes_load_data("INS_H", "2013-2014"), by = "SEQN") %>%
  left_join(nhanes_load_data("TRIGLY_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("DEMO_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("BMX_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("OGTT_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("BPX_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("PAQ_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("DPQ_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("SLQ_H", "2013-2014"), by = "SEQN")

raw_data <- data_files %>% 
  select(SEQN, RIAGENDR, RIDAGEYR, RIDRETH3, BMXBMI, LBDHDD, LBDLDL, LBXTR, LBXIN, LBXGLT, BPXSY1, BPXDI1, BMXWAIST,  PAD680, DMDEDUC2, DMDMARTL, DPQ020, SLD010H,  DIQ010) 

raw_data <- raw_data[raw_data$DIQ010 != 3 & raw_data$DIQ010 != 7 & raw_data$DIQ010 != 9, ] %>%  mutate(RIAGENDR = as_factor(RIAGENDR), RIDRETH3 = as_factor(RIDRETH3),DMDEDUC2 = as_factor(DMDEDUC2), DMDMARTL = as_factor(DMDMARTL), DIQ010 = as_factor(DIQ010) ) %>% 
  drop_na(DIQ010) 
 
 colnames(raw_data) <- c("ID", "gender", "age", "race", "bmi", "hdl", "ldl", "triglyceride", "insulin", "glucose", "bp_systolic","bp_diastolic", "waist","lifestyle", "education", "married", "depression", "sleep", "diabetes") 
 
 levels(raw_data$diabetes)[1] <- "yes"
 levels(raw_data$diabetes)[2] <- "no"
```
------------------------------------------------------------------------
Training data:

```{r}
# Missing data omitted
diabetes_data <- na.omit(raw_data)

set.seed(1)
trainRows <- createDataPartition(diabetes_data$diabetes, p = 0.8, list = FALSE)


# training data
x <- diabetes_data[trainRows ,-c(1, 19)]
y <- diabetes_data$diabetes[trainRows]

# test data
x2 <- diabetes_data[-trainRows ,-c(1, 19)]
y2 <- diabetes_data$diabetes[-trainRows]
```

```{r testing, eval=FALSE}
play = raw_data %>% 
  select(-glucose) %>% na.omit()

mid_eda = mid_naomit %>% 
  dplyr::mutate(gender = case_when(
    gender == 1 ~ "Male",
    gender == 2 ~ "Female",
    TRUE ~ NA_character_
  )) %>% 
  dplyr::mutate(race = case_when(
    race == 1 ~ "Mexican_American",
    race == 2	~ "Other_Hispanic",
    race == 3	~ "Non_Hispanic_White",
    race == 4	~ "Non_Hispanic_Black",
    race == 6	~ "Non_Hispanic_Asian",
    race == 7	~ "Other_Race_Including_MultiRacial",
    race == "."	~ "Missing",
    TRUE ~ NA_character_
  ))

```


```{r hw 4 and 5 code chunks}
## HW 3 CV trees
data("Prostate")
dat = Prostate %>% na.omit()

set.seed(1)
tree1 = rpart(formula = lpsa ~ . , 
               data = dat,
               control = rpart.control(cp = 0.01))

rpart.plot(tree1)
printcp(tree1)

# minimum cross-validation error
cpTable = tree1$cptable
minErr = which.min(cpTable[,4])

tree2 = prune(tree1, cp = cpTable[minErr,1])
rpart.plot(tree2)
plot(as.party(tree2))
summary(tree2)
tree2$cptable

## 1SE rule tree
tree3 = prune(tree1, cp = cpTable[cpTable[,4]<cpTable[minErr,4]+cpTable[minErr,5],1][1])
rpart.plot(tree3)
plot(as.party(tree3))
summary(tree3)
tree3$cptable

## more cv error codes
juice$Purchase = factor(juice$Purchase, c("CH", "MM"))


jtrain = createDataPartition(y = juice$Purchase,
                                p = 800/1070,
                                list = FALSE)

set.seed(1)
ojtree1 = rpart(formula = Purchase ~ . , 
              data = juice,
              subset = jtrain,
              control = rpart.control(cp = 0.01))

rpart.plot(ojtree1)
printcp(ojtree1)

# minimum cross-validation error
cpTable = ojtree1$cptable
minErr = which.min(cpTable[,4])

ojtree2 = prune(ojtree1, cp = cpTable[minErr,1])
rpart.plot(ojtree2)
plot(as.party(ojtree2))
summary(ojtree2)
ojtree2$cptable

oj_pred = predict(ojtree1, newdata = juice[-jtrain,], type = "prob")[,1]

oj_test_pred = rep("MM", length(oj_pred))
oj_test_pred[oj_pred>0.5] = "CH"

cvconf = confusionMatrix(data = as.factor(oj_test_pred),
                reference = juice$Purchase[-jtrain],
                positive = "CH")

cverr = ((cvconf$table[1,2]+cvconf$table[2,1])/(cvconf$table[1,1]+cvconf$table[1,2]+cvconf$table[2,1]+cvconf$table[2,2]))



## Bagging - like random forest but mtry is set to max
set.seed(1)
bagging = randomForest(lpsa ~ . , 
                        dat,
                        mtry = 8)

bag_vals = bagging$importance


## random forest
set.seed(1)
rf = randomForest(lpsa ~ . , 
                   dat,
                   mtry = 2)

rf_vals=rf$importance

rf_grid = expand.grid(mtry = 1:8,
                       splitrule = "variance",
                       min.node.size = 1:6)

## random forest in caret
set.seed(1)
rf_fit = train(lpsa ~ . , 
                dat, 
                method = "ranger",
                tuneGrid = rf_grid,
                trControl = ctrl)

## more random forest codes
ctrl = trainControl(method = "cv",
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)

rf_grid = expand.grid(mtry = 1:17,
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10, by = 2))

set.seed(1)
rf_class = train(Purchase ~ . , 
                juice, 
                subset = jtrain,
                method = "ranger",
                tuneGrid = rf_grid,
                metric = "ROC",
                trControl = ctrl)

ggplot(rf_class, highlight = TRUE)

set.seed(1)
rfclass_final = ranger(Purchase ~ . , 
                        juice[jtrain,],
                        mtry = rf_class$bestTune[[1]], 
                        min.node.size = rf_class$bestTune[[3]],
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

rf_table=rfclass_final$variable.importance

rfclass_pred = predict(rfclass_final, data = juice[-jtrain,], type = "response")$predictions

rfconf = confusionMatrix(data = as.factor(rfclass_pred),
                reference = juice$Purchase[-jtrain],
                positive = "CH")

rf_err = (rfconf$table[1,2]+rfconf$table[2,1])/(rfconf$table[1,1]+rfconf$table[1,2]+rfconf$table[2,1]+rfconf$table[2,2])

## Boosting
set.seed(1)
boost = gbm(lpsa ~ . , 
           dat,
           distribution = "gaussian",
           n.trees = 5000, 
           interaction.depth = 4,
           shrinkage = 0.005,
           cv.folds = 10, 
           n.cores = 5)
gbm.perf(boost, method = "cv")

gbm_grid = expand.grid(n.trees = c(0,1000,2000,3000,4000,5000,6000),
                        interaction.depth = 1:4,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = c(1,10))

ctrl = trainControl(method = "cv") 
set.seed(1)
gbm_fit = train(lpsa ~ . , 
                 dat, 
                 method = "gbm",
                 tuneGrid = gbm_grid,
                 trControl = ctrl,
                 verbose = FALSE)

ggplot(gbm_fit, highlight = TRUE)

summary(gbm_fit$finalModel)

## more boosting codes
gbm_grid = expand.grid(n.trees = c(0,2000,4000,6000),
                        interaction.depth = 1:4,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = c(1,10))

ctrl2 = trainControl(method = "cv") 
set.seed(1)
gbm2_fit = train(Purchase ~ . , 
                 juice, 
                subset = jtrain,
                 method = "gbm",
                 tuneGrid = gbm_grid,
                 trControl = ctrl2,
                 verbose = FALSE)

ggplot(gbm2_fit, highlight = TRUE)

summary(gbm2_fit$finalModel)

gbm2_pred = predict(gbm2_fit, newdata = juice[-jtrain,], type = "prob")[,1]

gbm2_test_pred = rep("MM", length(gbm2_pred))
gbm2_test_pred[gbm2_pred>0.5] = "CH"

bstconf = confusionMatrix(data = as.factor(gbm2_test_pred),
                reference = juice$Purchase[-jtrain],
                positive = "CH")

bstconf$table

bst_err = (bstconf$table[1,2]+bstconf$table[2,1])/(bstconf$table[1,1]+bstconf$table[1,2]+bstconf$table[2,1]+bstconf$table[2,2])


## linear kernels
set.seed(1)
linear_tune = tune.svm(Purchase ~ . , 
                        juice, 
                        data = juice[jtrain,],
                        kernel = "linear", 
                        cost = exp(seq(-4,4,len=50)),
                        scale = TRUE)
plot(linear_tune)

#linear_tune$best.parameters
lintrain_err = linear_tune$best.performance 

best_linear = linear_tune$best.model

pred_linear = predict(best_linear, newdata = juice[-jtrain,])

lin_conf = confusionMatrix(data = pred_linear, 
                reference = juice$Purchase[-jtrain])

lin_err = (lin_conf$table[1,2]+lin_conf$table[2,1])/(lin_conf$table[1,1]+lin_conf$table[1,2]+lin_conf$table[2,1]+lin_conf$table[2,2])

#lin_err


## radial kernels
set.seed(1)
radial_tune = tune.svm(Purchase ~ . , 
                        data = juice[jtrain,], 
                        kernel = "radial", 
                        cost = exp(seq(-4,4,len=10)),
                        gamma = exp(seq(-6,-2,len=10)))

plot(radial_tune, transform.y = log, transform.x = log, 
     color.palette = terrain.colors)
# summary(radial_tune)

best_radial = radial_tune$best.model
#summary(best_radial)
radtrain_err = radial_tune$best.performance

pred_radial = predict(best_radial, newdata = juice[-jtrain,])

rad_conf = confusionMatrix(data = pred_radial, 
                reference = juice$Purchase[-jtrain])

rad_err = (rad_conf$table[1,2]+rad_conf$table[2,1])/(rad_conf$table[1,1]+rad_conf$table[1,2]+rad_conf$table[2,1]+rad_conf$table[2,2])

```



