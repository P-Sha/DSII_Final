---
title: "Diabetes Prediction model" 
author: "DS II Final team"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, results = 'asis')
```

\newpage

```{r}
library(RNHANES)
library(tidyverse)
library(summarytools)
library(leaps)
library(readr)
library(caret)
library(ggplot2)
library(patchwork)
library(mgcv)
library(nlme)
library(dplyr)
library(plyr)
library(AppliedPredictiveModeling)
library(dplyr)
library(scales)
library(pROC)
#library(MASS) 
#library(klaR)
library(forcats)
library(visdat)
library(glmnet)
library(mlbench)
library(pROC)
library(pdp)
library(vip)
library(rpart.plot)
library(ranger)
```

# Load Data

```{r}
data_files <- nhanes_load_data(file_name = "DIQ_H", year = "2013-2014")

data_files <- data_files %>% 
  left_join(nhanes_load_data("HDL_H", "2013-2014"), by = "SEQN") %>%
  left_join(nhanes_load_data("INS_H", "2013-2014"), by = "SEQN") %>%
  left_join(nhanes_load_data("TRIGLY_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("DEMO_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("BMX_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("OGTT_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("BPX_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("PAQ_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("DPQ_H", "2013-2014"), by = "SEQN") %>% 
  left_join(nhanes_load_data("SLQ_H", "2013-2014"), by = "SEQN")
  

raw_data <- data_files %>% 
  select(SEQN, RIAGENDR, RIDAGEYR, RIDRETH3, BMXBMI, LBDHDD, LBDLDL, LBXTR, LBXIN, LBXGLT, BPXSY1, BPXDI1, BMXWAIST,  PAD680, DMDEDUC2, DMDMARTL, DPQ020, SLD010H,  DIQ010) 

raw_data <- raw_data[raw_data$DIQ010 != 3 & raw_data$DIQ010 != 7 & raw_data$DIQ010 != 9, ] %>%  mutate(RIAGENDR = as_factor(RIAGENDR), RIDRETH3 = as_factor(RIDRETH3),DMDEDUC2 = as_factor(DMDEDUC2), DMDMARTL = as_factor(DMDMARTL), DIQ010 = as_factor(DIQ010) ) %>% 
  drop_na(DIQ010) 
 
 colnames(raw_data) <- c("ID", "gender", "age", "race", "bmi", "hdl", "ldl", "triglyceride", "insulin", "glucose", "bp_systolic","bp_diastolic", "waist","lifestyle", "education", "married", "depression", "sleep", "diabetes") 
 
 contrasts(raw_data$diabetes)
 levels(raw_data$diabetes)[1] <- "yes"
 levels(raw_data$diabetes)[2] <- "no"
  contrasts(raw_data$diabetes)
  
write.csv(raw_data, "final_data.csv")
```


# EDA

## Summary statistics

```{r}
st_options(plain.ascii = FALSE,       
           style = "rmarkdown", 
           dfSummary.silent = TRUE,        
           footnote = NA,          
           subtitle.emphasis = FALSE)      

dfSummary(raw_data[,-1], valid.col = FALSE)

# Delete high missing-data covariates
raw_data <- raw_data[-c(7:10)]
dfSummary(raw_data[,-1], valid.col = FALSE)
```

## Density plots (numerical covariates)

```{r, fig.height=4}

theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)

raw_data <- raw_data %>% 
  select(married, everything()) %>% 
  select(education, everything()) %>% 
  select(race, everything()) %>% 
  select(gender, everything()) %>% 
  select(ID, everything())

featurePlot(x = raw_data[, 6:14], 
            y = raw_data$diabetes,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))
```

## Bar plots (categorical covariates)

```{r}
diabetes_gender = ggplot(raw_data, 
       aes(x = diabetes, 
           fill = factor(gender,
                         levels = c("1", "2"),
                         labels = c("male", "female")))) + 
  geom_bar(position = position_dodge(preserve = "single")) +
   scale_fill_brewer(palette = "Set2") +
  labs(fill = "gender")

diabetes_race = ggplot(raw_data, 
       aes(x = diabetes, 
           fill = factor(race,
                         levels = c("1", "2", "3", "4", "6", "7"),
                         labels = c("Mexican American", "Other Hispanic", "White", "Black", "Asian", "Other")))) + 
  geom_bar(position = position_dodge(preserve = "single")) +
   scale_fill_brewer(palette = "Set2") +
   labs(fill = "race")

diabetes_education = ggplot(raw_data, 
       aes(x = diabetes, 
           fill = factor(education,
                         levels = c("1", "2", "3", "4", "5"),
                         labels = c("< 9th grade", "9 - 11 grade", "H.S./GED", "Some college", "College graduate")))) + 
  geom_bar(position = position_dodge(preserve = "single")) +
   scale_fill_brewer(palette = "Set2") +
   labs(fill = "education")

diabetes_married = ggplot(raw_data, 
       aes(x = diabetes, 
           fill = factor(married,
                         levels = c("1", "2", "3", "4", "5", "6"),
                         labels = c("Married", "Widowed", "Divorced", "Separated", "Never married", "Living with partner")))) + 
  geom_bar(position = position_dodge(preserve = "single")) +
   scale_fill_brewer(palette = "Set2") +
    labs(fill = "married")

(diabetes_gender + diabetes_race)  / (diabetes_education + diabetes_married) 
```

## Partition-plots

```{r}
set.seed(1)
rowTrain <- createDataPartition(y = raw_data$diabetes,
                                p = 0.7,
                                list = FALSE)

# Exploratory analysis: LDA/QDA/NB based on every combination of two variables

# klaR::partimat(diabetes ~ age + bmi +  hdl + bp_systolic + waist, 
#        data = raw_data, subset = rowTrain, method = "lda")

# klaR::partimat(diabetes ~ age + bmi + hdl  + bp_systolic + waist, 
#         data = raw_data, subset = rowTrain, method = "qda")

klaR::partimat(diabetes ~ age + bmi + hdl  + bp_systolic + waist, 
          data = raw_data, subset = rowTrain, method = "naiveBayes")
```

# Models

## Prep/partition data

```{r}
# Omit Missing data
diabetes_data <- na.omit(raw_data)

# Omit low-count subcategories
diabetes_data <- na.omit(diabetes_data) %>%
  filter(married != "77") %>%
  filter(education != "7") %>%
  filter(education != "9") %>%
  droplevels()

set.seed(1)
trainRows <- createDataPartition(diabetes_data$diabetes, p = 0.8, list = FALSE)


# training data
x <- diabetes_data[trainRows ,-c(1, 15)]
y <- diabetes_data$diabetes[trainRows]

# test data
x2 <- diabetes_data[-trainRows ,-c(1, 15)]
y2 <- diabetes_data$diabetes[-trainRows]

# Setup CV method
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
```

## Linear models

```{r}
# glm
set.seed(1)

model.glm <- train(x = x, 
                   y = y,
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl)

# glm.pred <- predict(model.glm, newdata = x2, type = "prob")[,2]
# roc.glm <- roc(y2, glm.pred)
# plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
# plot(smooth(roc.glm), col = 4, add = TRUE)

# Penalized Logistic regression
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 6),
                        .lambda = exp(seq(-8, -2, length = 10)))
set.seed(1)

model.glmn <- train(x = data.matrix(x),
                    y = y,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)

plot(model.glmn, xTrans = function(x)log(x))   

model.glmn$bestTune

# glmn.pred <- predict(model.glmn, newdata = data.matrix(x2), type = "prob")[,2]
# roc.glmn <- roc(y2, glmn.pred)
# plot(roc.glmn, legacy.axes = TRUE, print.auc = TRUE)
# plot(smooth(roc.glmn), col = 4, add = TRUE)

# LDA
# set.seed(1)

# model.lda <- train(x = data.matrix(x),
#                   y = y,
#                   method = "lda",
#                   metric = "ROC",
#                   trControl = ctrl)

# lda.pred <- predict(model.lda, newdata = data.matrix(x2), type = "prob") [,2]

# roc.lda <- roc(y2, lda.pred)
# plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
# plot(smooth(roc.lda), col = 4, add = TRUE)
```

## Nonlinear models

```{r}
## Non-linear Logistic regression: GAM, MARS
# GAM
#set.seed(1)
#model.gam <- train(x = x,
#                   y = y,
#                   method = "gam",
#                   metric = "ROC",
#                   trControl = ctrl)

#model.gam$finalModel

# MARS
set.seed(1)

model.mars <- train(x = x,
                    y = y,
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3, 
                                           nprune = 2:15),
                    metric = "ROC",
                    trControl = ctrl)

plot(model.mars)

coef(model.mars$finalModel) 

## Non-linear Discriminant analysis: QDA, Naive Bayes (NB)
# QDA = for continuous features
#set.seed(1)
#model.qda <- train(x = x,
#                   y = y,
#                   method = "qda",
#                   metric = "ROC",
#                   trControl = ctrl)

# NB
set.seed(1)

nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1, 
                      adjust = seq(.2, 2.5, by = .2))

model.nb <- train(x = x,
                  y = y,
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl)

plot(model.nb)
```

## trees/ SVM

```{r}
## single tree. very useless
#set.seed(1)
#rpart.fit <- train(diabetes ~ . , 
#                   diabetes_data[trainRows,-1], 
#                   method = "rpart",
#                   tuneGrid = data.frame(cp = exp(seq(-1,10, length = 500))),
#                   trControl = ctrl)
#ggplot(rpart.fit, highlight = TRUE)
#rpart.plot(rpart.fit$finalModel)

## random forest in caret

#rf_grid = expand.grid(mtry = 1:13,
#                       splitrule = "gini",
#                       min.node.size = seq(from = 2, to = 10, by = 2))

#set.seed(1)
#rf.fit = train(diabetes ~ . ,
#               diabetes_data[trainRows,-1], 
#               method = "ranger",
#               tuneGrid = rf_grid,
#               metric = "ROC",
#               trControl = ctrl)

#ggplot(rf.fit, highlight = TRUE)

#set.seed(1)
#rf_final = ranger(diabetes ~ . ,
#                  diabetes_data[trainRows,-1],
#                  mtry = rf_fit$bestTune[[1]],
#                  min.node.size = rf_fit$bestTune[[3]],
#                  importance = "permutation",
#                  scale.permutation.importance = TRUE) 
#rf_table=rf_final$variable.importance
#rf_final$prediction.error
#rfclass_pred = predict(rf_final, data = diabetes_data[-trainRows,-1], type = "response")$predictions
#rfconf = confusionMatrix(data = as.factor(rfclass_pred),
#               reference = y2,
#                positive = "yes")

#rf_err = (rfconf$table[1,2]+rfconf$table[2,1])/(rfconf$table[1,1]+rfconf$table[1,2]+rfconf$table[2,1]+rfconf$table[2,2])

### gbm/gbma
#gbm_grid = expand.grid(n.trees = c(0,1000,2000,3000,4000,5000,6000),
#                        interaction.depth = 1:4,
#                        shrinkage = c(0.001,0.003,0.005),
#                        n.minobsinnode = c(1,10))

#set.seed(1)
#gbm_fit = train(diabetes ~ . , 
#                 diabetes_data[trainRows,-1], 
#                 method = "gbm",
#                 tuneGrid = gbm_grid,
#                 trControl = ctrl,
#                 verbose = FALSE)
#ggplot(gbm_fit, highlight = TRUE)
#summary(gbm_fit$finalModel)
#gbm_pred <- predict(gbm_fit, newdata = diabetes_data[-trainRows,], type = "prob")[,1]
#gbm_test_pred = rep("no", length(gbm_pred))
#gbm_test_pred[gbm_pred>0.5] = "yes"
#gbmconf = confusionMatrix(data = as.factor(gbm_test_pred),
#                reference = diabetes_data$diabetes[-trainRows],
#                positive = "yes")
#gbmconf$table
#gbm_err = (gbmconf$table[1,2]+gbmconf$table[2,1])/(gbmconf$table[1,1]+gbmconf$table[1,2]+gbmconf$table[2,1]+gbmconf$table[2,2])

gbmA_grid <- expand.grid(n.trees = c(2000,3000,4000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.001,0.003,0.005),
                         n.minobsinnode = 1)

set.seed(1)
gbmA.fit <- train(diabetes ~ . , 
                  diabetes_data, 
                  subset = trainRows, 
                  tuneGrid = gbmA_grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)
ggplot(gbmA.fit, highlight = TRUE)
#gbmA_pred <- predict(gbmA_fit, newdata = diabetes_data[-trainRows,], type = "prob")[,1]
#gbmA_test_pred = rep("no", length(gbmA_pred))
#gbmA_test_pred[gbmA_pred>0.5] = "yes"
#gbmAconf = confusionMatrix(data = as.factor(gbmA_test_pred),
#                reference = diabetes_data$diabetes[-trainRows],
#                positive = "yes")
#gbmAconf$table
#gbmA_err = (gbmAconf$table[1,2]+gbmAconf$table[2,1])/(gbmAconf$table[1,1]+gbmAconf$table[1,2]+gbmAconf$table[2,1]+gbmAconf$table[2,2])

# Comparing Ensemble methods
#res <- resamples(list(rf = rf.fit, 
#                      gbm = gbm_fit, 
#                      gbmA = gbmA.fit ))

#summary(res)
#bwplot(res, metric = "ROC")


## SVML/R
#ctrl <- trainControl(method = "cv")
# kernlab
set.seed(1)
svml.fit <- train(diabetes ~ . , 
                  data = diabetes_data[trainRows,], 
                  method = "svmLinear",
                  #preProcess = c("center", "scale"),
                  tuneGrid = data.frame(C = exp(seq(0,2,len = 100))),
                  metric = "ROC",
                  trControl = ctrl)
plot(svml.fit, highlight = TRUE, xTrans = log)
#pred.svml <- predict(svml.fit, newdata = diabetes_data[-trainRows,])
#confusionMatrix(data = pred.svml, 
#                reference = diabetes_data$diabetes[-trainRows])

## radial
#svmr.grid <- expand.grid(C = exp(seq(-1,3,len = 10)),
#                         sigma = exp(seq(-4,0,len = 10)))

# tunes over both cost and sigma
#set.seed(1)             
#svmr.fit <- train(diabetes ~ . , 
#                  diabetes_data, 
#                  subset = trainRows,
#                  method = "svmRadialSigma",
#                  preProcess = c("center", "scale"),
#                  tuneGrid = svmr.grid,
#                  trControl = ctrl)
#plot(svmr.fit, highlight = TRUE)
#pred.svmr <- predict(svmr.fit, newdata = diabetes_data[-trainRows,])
#confusionMatrix(data = pred.svmr, 
#                reference = diabetes_data$diabetes[-trainRows])

# Comparing svm methods
#res <- resamples(list(svml = svml.fit, 
#                      svmr = svmr.fit))

#summary(res)
#bwplot(res, metric = "ROC")


```

## Model comparison

```{r}
res <- resamples(list(GLM = model.glm, 
                      GLMNET = model.glmn, 
                      MARS = model.mars, 
                      NB = model.nb,
                      gbmA = gbmA.fit,
                      svml = svml.fit))

summary(res)
bwplot(res, metric = "ROC")
```


